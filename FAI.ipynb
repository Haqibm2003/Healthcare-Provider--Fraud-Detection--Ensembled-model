{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROi9HBUgJxTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981bc91f-e873-4985-f305-f6dbf7be4b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "HEALTHCARE FRAUD DETECTION - FINAL WORKING VERSION\n",
            "================================================================================\n",
            "\n",
            "üöÄ Starting execution...\n",
            "\n",
            "üì¶ Installing packages...\n",
            "‚úÖ Packages installed!\n",
            "\n",
            "‚úÖ Libraries imported!\n",
            "\n",
            "================================================================================\n",
            "STEP 1: LOADING DATA\n",
            "================================================================================\n",
            "‚úÖ Loaded 138,556 train beneficiaries\n",
            "‚úÖ Loaded 63,968 test beneficiaries\n",
            "\n",
            "================================================================================\n",
            "STEP 2: FIXING LABELS\n",
            "================================================================================\n",
            "\n",
            "üîç Train labels: ['Provider', 'PotentialFraud']\n",
            "üîç Test labels: ['Provider']\n",
            "‚ö†Ô∏è  Test labels missing - will use train/val split\n",
            "\n",
            "üîß Converting chronic conditions...\n",
            "‚úÖ Conversions done\n",
            "\n",
            "================================================================================\n",
            "STEP 3: DATA OVERVIEW\n",
            "================================================================================\n",
            "\n",
            "üìä Fraud Distribution:\n",
            "   No: 4,904 (90.6%)\n",
            "   Yes: 506 (9.4%)\n",
            "\n",
            "‚úÖ Overview saved\n",
            "\n",
            "================================================================================\n",
            "STEP 4: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "\n",
            "üîÑ TRAIN...\n",
            "   ‚úÖ 29 features for 5,410 providers\n",
            "\n",
            "   Split: 4,328 train, 1,082 validation\n",
            "\n",
            "================================================================================\n",
            "STEP 5: PREPROCESSING\n",
            "================================================================================\n",
            "‚úÖ Missing values handled\n",
            "‚úÖ Target encoded\n",
            "üíæ Data saved\n",
            "\n",
            "================================================================================\n",
            "STEP 6: MODELING\n",
            "================================================================================\n",
            "\n",
            "üìä Train: (4328, 29), Fraud: 401 (9.3%)\n",
            "üìä Validation: (1082, 29), Fraud: 105 (9.7%)\n",
            "\n",
            "‚úÖ Scaled\n",
            "‚úÖ SMOTE: 3,927 fraud, 3,927 non-fraud\n",
            "\n",
            "================================================================================\n",
            "STEP 7: TRAINING MODELS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Model 1/3: Logistic Regression\n",
            "‚úÖ 0.07s | Acc: 87.80% | F1: 0.5658\n",
            "\n",
            "================================================================================\n",
            "Model 2/3: Random Forest\n",
            "‚úÖ 4.37s | Acc: 90.67% | F1: 0.5628\n",
            "\n",
            "================================================================================\n",
            "Model 3/3: SVM\n",
            "‚úÖ 9.50s | Acc: 87.52% | F1: 0.5603\n",
            "\n",
            "================================================================================\n",
            "STEP 8: RESULTS\n",
            "================================================================================\n",
            "\n",
            "              Model  Accuracy  Precision   Recall       F1      AUC\n",
            "Logistic Regression  0.878004   0.432161 0.819048 0.565789 0.929278\n",
            "      Random Forest  0.906654   0.515873 0.619048 0.562771 0.911727\n",
            "                SVM  0.875231   0.425743 0.819048 0.560261 0.914568\n",
            "\n",
            "üèÜ BEST MODEL: Logistic Regression\n",
            "   Accuracy:  87.80%\n",
            "   Precision: 43.22%\n",
            "   Recall:    81.90%\n",
            "   F1-Score:  0.5658\n",
            "   ROC-AUC:   0.9293\n",
            "\n",
            "üìä Top 15 Features:\n",
            "    1. InscClaimAmtReimbursed_sum               0.156494\n",
            "   16. UniqueProc_sum                           0.105951\n",
            "    5. DeductibleAmtPaid_sum                    0.096208\n",
            "    4. InscClaimAmtReimbursed_max               0.071433\n",
            "   14. UniqueDiag_sum                           0.071229\n",
            "    8. DeductibleAmtPaid_max                    0.057665\n",
            "   10. TotalClaims                              0.041830\n",
            "    7. DeductibleAmtPaid_std                    0.034185\n",
            "    9. UniqueBeneficiaries                      0.034102\n",
            "    6. DeductibleAmtPaid_mean                   0.030044\n",
            "   17. UniqueProc_mean                          0.027277\n",
            "    3. InscClaimAmtReimbursed_std               0.023196\n",
            "    2. InscClaimAmtReimbursed_mean              0.020822\n",
            "   12. NumOperatingPhysicians                   0.017482\n",
            "   11. NumAttendingPhysicians                   0.015693\n",
            "\n",
            "üíæ Models saved\n",
            "\n",
            "================================================================================\n",
            "STEP 9: CREATING VISUALIZATIONS\n",
            "================================================================================\n",
            "   ‚úÖ model_comparison.png\n",
            "   ‚úÖ confusion_matrix.png\n",
            "   ‚úÖ roc_curves.png\n",
            "   ‚úÖ feature_importance.png\n",
            "   ‚úÖ class_distribution.png\n",
            "\n",
            "================================================================================\n",
            "üéâ PROJECT COMPLETED!\n",
            "================================================================================\n",
            "\n",
            "üìä Summary:\n",
            "   Training: 4,328 providers, 29 features\n",
            "   Validation: 1,082 providers\n",
            "\n",
            "üíæ Generated Files (10):\n",
            "   üìä Analytics: 1 dashboard\n",
            "   üìà ML Visualizations: 5 charts\n",
            "   üìÅ Data & Models: 4 files\n",
            "\n",
            "üéì Mohammed Haqib (RA2512049015044)\n",
            "üè´ SRM Institute of Science and Technology\n",
            "üìÖ October 2025\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# HEALTHCARE FRAUD DETECTION - COMPLETE WORKING CODE\n",
        "# Author: Mohammed Haqib (RA2512049015044)\n",
        "# Institution: SRM Institute of Science and Technology\n",
        "# COPY-PASTE THIS ENTIRE CODE AND RUN - GUARANTEED TO WORK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HEALTHCARE FRAUD DETECTION - FINAL WORKING VERSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nüöÄ Starting execution...\\n\")\n",
        "\n",
        "# Install packages\n",
        "import subprocess, sys\n",
        "print(\"üì¶ Installing packages...\")\n",
        "for pkg in ['imbalanced-learn', 'scikit-learn', 'seaborn>=0.12.0']:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "    except: pass\n",
        "print(\"‚úÖ Packages installed!\\n\")\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings, os, pickle\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import *\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 1: LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_ben = pd.read_csv('Train_Beneficiarydata-1542865627584.csv')\n",
        "train_inp = pd.read_csv('Train_Inpatientdata-1542865627584.csv')\n",
        "train_out = pd.read_csv('Train_Outpatientdata-1542865627584.csv')\n",
        "train_lab = pd.read_csv('Train-1542865627584.csv')\n",
        "\n",
        "test_ben = pd.read_csv('Test_Beneficiarydata-1542969243754.csv')\n",
        "test_inp = pd.read_csv('Test_Inpatientdata-1542969243754.csv')\n",
        "test_out = pd.read_csv('Test_Outpatientdata-1542969243754.csv')\n",
        "test_lab = pd.read_csv('Test-1542969243754.csv')\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(train_ben):,} train beneficiaries\")\n",
        "print(f\"‚úÖ Loaded {len(test_ben):,} test beneficiaries\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: FIX LABELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 2: FIXING LABELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüîç Train labels: {list(train_lab.columns)}\")\n",
        "print(f\"üîç Test labels: {list(test_lab.columns)}\")\n",
        "\n",
        "# Find fraud column\n",
        "poss = ['PotentialFraud', 'Potential Fraud', 'potential_fraud', 'fraud', 'Fraud', 'is_fraud', 'IsFraud']\n",
        "fraud_col = next((c for c in train_lab.columns if c in poss),\n",
        "                 train_lab.columns[1] if len(train_lab.columns) >= 2 else None)\n",
        "\n",
        "if fraud_col and fraud_col != 'PotentialFraud':\n",
        "    print(f\"üîß Renaming '{fraud_col}' ‚Üí 'PotentialFraud'\")\n",
        "    train_lab.rename(columns={fraud_col: 'PotentialFraud'}, inplace=True)\n",
        "\n",
        "# Check test labels\n",
        "has_test_labels = len(test_lab.columns) >= 2\n",
        "if has_test_labels:\n",
        "    test_fraud_col = next((c for c in test_lab.columns if c in poss), test_lab.columns[1])\n",
        "    if test_fraud_col != 'PotentialFraud':\n",
        "        test_lab.rename(columns={test_fraud_col: 'PotentialFraud'}, inplace=True)\n",
        "    print(\"‚úÖ Test has labels\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Test labels missing - will use train/val split\")\n",
        "\n",
        "# Convert chronic Y/N to 1/0\n",
        "print(\"\\nüîß Converting chronic conditions...\")\n",
        "for df in [train_ben, test_ben]:\n",
        "    chrs = [c for c in df.columns if 'ChronicCond' in c or 'RenalDiseaseIndicator' in c]\n",
        "    for col in chrs:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].map({'Y': 1, 'y': 1, 'Yes': 1, 'N': 0, 'n': 0, 'No': 0, 1: 1, 0: 0, 2: 1}).fillna(0)\n",
        "print(\"‚úÖ Conversions done\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: DATA OVERVIEW\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 3: DATA OVERVIEW\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "os.makedirs('analytics', exist_ok=True)\n",
        "\n",
        "fraud_cnt = train_lab['PotentialFraud'].value_counts()\n",
        "print(f\"\\nüìä Fraud Distribution:\")\n",
        "for k, v in fraud_cnt.items():\n",
        "    print(f\"   {k}: {v:,} ({v/len(train_lab)*100:.1f}%)\")\n",
        "\n",
        "# Dashboard\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('Dataset Overview', fontsize=18, fontweight='bold')\n",
        "\n",
        "axes[0, 0].pie(fraud_cnt.values, labels=fraud_cnt.index, autopct='%1.1f%%',\n",
        "               colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
        "axes[0, 0].set_title('Fraud Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "claim_types = pd.DataFrame({'Type': ['Inpatient', 'Outpatient'],\n",
        "                             'Count': [len(train_inp), len(train_out)]})\n",
        "axes[0, 1].bar(claim_types['Type'], claim_types['Count'], color=['#3498db', '#9b59b6'])\n",
        "axes[0, 1].set_title('Claims by Type', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Count', fontweight='bold')\n",
        "for i, v in enumerate(claim_types['Count']):\n",
        "    axes[0, 1].text(i, v+5000, f'{v:,}', ha='center', fontweight='bold')\n",
        "axes[0, 1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "prov_stat = pd.DataFrame({'Dataset': ['Train', 'Test'],\n",
        "                          'Providers': [train_lab['Provider'].nunique(), test_lab['Provider'].nunique()]})\n",
        "axes[1, 0].bar(prov_stat['Dataset'], prov_stat['Providers'], color=['#e67e22', '#16a085'])\n",
        "axes[1, 0].set_title('Provider Count', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Count', fontweight='bold')\n",
        "for i, v in enumerate(prov_stat['Providers']):\n",
        "    axes[1, 0].text(i, v+50, f'{v:,}', ha='center', fontweight='bold')\n",
        "axes[1, 0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "data_sz = pd.DataFrame({'Component': ['Train\\nBen', 'Test\\nBen', 'Train\\nClaims', 'Test\\nClaims'],\n",
        "                        'Count': [len(train_ben), len(test_ben),\n",
        "                                  len(train_inp)+len(train_out), len(test_inp)+len(test_out)]})\n",
        "axes[1, 1].bar(data_sz['Component'], data_sz['Count'], color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])\n",
        "axes[1, 1].set_title('Dataset Size', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Count', fontweight='bold')\n",
        "for i, v in enumerate(data_sz['Count']):\n",
        "    axes[1, 1].text(i, v+5000, f'{v:,}', ha='center', fontweight='bold', fontsize=9)\n",
        "axes[1, 1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('analytics/overview.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\n‚úÖ Overview saved\")\n",
        "plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def create_features(ben, inp, out, name):\n",
        "    print(f\"\\nüîÑ {name}...\")\n",
        "\n",
        "    inp_c = inp.copy()\n",
        "    out_c = out.copy()\n",
        "    inp_c['ClaimType'] = 'Inpatient'\n",
        "    out_c['ClaimType'] = 'Outpatient'\n",
        "\n",
        "    common = list(set(inp_c.columns) & set(out_c.columns))\n",
        "    claims = pd.concat([inp_c[common], out_c[common]], ignore_index=True)\n",
        "    claims_ben = claims.merge(ben, on='BeneID', how='left')\n",
        "\n",
        "    # Financial\n",
        "    fin_cols = [c for c in ['InscClaimAmtReimbursed', 'DeductibleAmtPaid'] if c in claims_ben.columns]\n",
        "    if fin_cols:\n",
        "        agg = {c: ['sum', 'mean', 'std', 'max'] for c in fin_cols}\n",
        "        features = claims_ben.groupby('Provider').agg(agg).reset_index()\n",
        "        features.columns = ['_'.join(c).strip('_') if c[1] else c[0] for c in features.columns.values]\n",
        "    else:\n",
        "        features = pd.DataFrame({'Provider': claims_ben['Provider'].unique()})\n",
        "\n",
        "    # Utilization\n",
        "    util = claims_ben.groupby('Provider').agg({'BeneID': 'nunique', 'ClaimID': 'count'}).reset_index()\n",
        "    util.columns = ['Provider', 'UniqueBeneficiaries', 'TotalClaims']\n",
        "    features = features.merge(util, on='Provider', how='outer')\n",
        "\n",
        "    # Physician\n",
        "    phys = [c for c in ['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician'] if c in claims_ben.columns]\n",
        "    if phys:\n",
        "        phys_feat = claims_ben.groupby('Provider')[phys].nunique().reset_index()\n",
        "        phys_feat.columns = ['Provider'] + [f'Num{c}s' for c in phys]\n",
        "        features = features.merge(phys_feat, on='Provider', how='outer')\n",
        "\n",
        "    # Clinical\n",
        "    diag = [c for c in claims_ben.columns if 'ClmDiagnosisCode' in c]\n",
        "    proc = [c for c in claims_ben.columns if 'ClmProcedureCode' in c]\n",
        "    claims_ben['UniqueDiag'] = claims_ben[diag].apply(lambda x: x.dropna().nunique(), axis=1) if diag else 0\n",
        "    claims_ben['UniqueProc'] = claims_ben[proc].apply(lambda x: x.dropna().nunique(), axis=1) if proc else 0\n",
        "\n",
        "    clin = claims_ben.groupby('Provider')[['UniqueDiag', 'UniqueProc']].agg(['sum', 'mean']).reset_index()\n",
        "    clin.columns = ['_'.join(c).strip('_') if c[1] else c[0] for c in clin.columns.values]\n",
        "    features = features.merge(clin, on='Provider', how='outer')\n",
        "\n",
        "    # Chronic\n",
        "    chronic = [c for c in ben.columns if 'ChronicCond' in c or 'RenalDiseaseIndicator' in c]\n",
        "    if chronic:\n",
        "        claims_chr = claims_ben[['Provider', 'BeneID']].drop_duplicates().merge(\n",
        "            ben[['BeneID'] + chronic], on='BeneID', how='left')\n",
        "        chr_feat = claims_chr.groupby('Provider')[chronic].mean().reset_index()\n",
        "        features = features.merge(chr_feat, on='Provider', how='outer')\n",
        "\n",
        "    print(f\"   ‚úÖ {features.shape[1]-1} features for {len(features):,} providers\")\n",
        "    return features\n",
        "\n",
        "train_features = create_features(train_ben, train_inp, train_out, \"TRAIN\")\n",
        "\n",
        "# Merge labels\n",
        "train_final = train_features.merge(train_lab[['Provider', 'PotentialFraud']], on='Provider', how='left')\n",
        "\n",
        "# Handle test data\n",
        "if has_test_labels:\n",
        "    test_features = create_features(test_ben, test_inp, test_out, \"TEST\")\n",
        "    test_final = test_features.merge(test_lab[['Provider', 'PotentialFraud']], on='Provider', how='left')\n",
        "else:\n",
        "    # Split training data\n",
        "    train_prov = train_final[['Provider']].drop_duplicates()\n",
        "    train_p, val_p = train_test_split(train_prov, test_size=0.2, random_state=42)\n",
        "    test_final = train_final[train_final['Provider'].isin(val_p['Provider'])].copy()\n",
        "    train_final = train_final[train_final['Provider'].isin(train_p['Provider'])].copy()\n",
        "    print(f\"\\n   Split: {len(train_final):,} train, {len(test_final):,} validation\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "numeric = train_final.select_dtypes(include=[np.number]).columns\n",
        "numeric = [c for c in numeric if c != 'Provider']\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "train_final[numeric] = imputer.fit_transform(train_final[numeric])\n",
        "test_final[numeric] = imputer.transform(test_final[numeric])\n",
        "print(\"‚úÖ Missing values handled\")\n",
        "\n",
        "train_final['PotentialFraud'] = train_final['PotentialFraud'].replace({\n",
        "    'Yes': 1, 'yes': 1, 'YES': 1, 'Y': 1, 1: 1,\n",
        "    'No': 0, 'no': 0, 'NO': 0, 'N': 0, 0: 0\n",
        "})\n",
        "test_final['PotentialFraud'] = test_final['PotentialFraud'].replace({\n",
        "    'Yes': 1, 'yes': 1, 'YES': 1, 'Y': 1, 1: 1,\n",
        "    'No': 0, 'no': 0, 'NO': 0, 'N': 0, 0: 0\n",
        "})\n",
        "print(\"‚úÖ Target encoded\")\n",
        "\n",
        "train_final.to_csv('processed_train.csv', index=False)\n",
        "test_final.to_csv('processed_validation.csv', index=False)\n",
        "print(\"üíæ Data saved\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: MODELING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 6: MODELING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train = train_final.drop(['Provider', 'PotentialFraud'], axis=1)\n",
        "y_train = train_final['PotentialFraud']\n",
        "X_test = test_final.drop(['Provider', 'PotentialFraud'], axis=1)\n",
        "y_test = test_final['PotentialFraud']\n",
        "\n",
        "print(f\"\\nüìä Train: {X_train.shape}, Fraud: {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"üìä Validation: {X_test.shape}, Fraud: {(y_test==1).sum():,} ({(y_test==1).sum()/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_sc = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_sc = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "print(\"\\n‚úÖ Scaled\")\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train_sc, y_train)\n",
        "print(f\"‚úÖ SMOTE: {(y_train_bal==1).sum():,} fraud, {(y_train_bal==0).sum():,} non-fraud\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 7: TRAINING MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "trained = {}\n",
        "results = {}\n",
        "\n",
        "for idx, (name, model) in enumerate(models.items(), 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Model {idx}/3: {name}\")\n",
        "    start = datetime.now()\n",
        "    model.fit(X_train_bal, y_train_bal)\n",
        "    t = (datetime.now() - start).total_seconds()\n",
        "\n",
        "    trained[name] = model\n",
        "    y_pred = model.predict(X_test_sc)\n",
        "    y_proba = model.predict_proba(X_test_sc)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
        "        'AUC': roc_auc_score(y_test, y_proba) if y_proba is not None else 0,\n",
        "        'Pred': y_pred,\n",
        "        'Proba': y_proba\n",
        "    }\n",
        "\n",
        "    print(f\"‚úÖ {t:.2f}s | Acc: {results[name]['Accuracy']*100:.2f}% | F1: {results[name]['F1']:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comp = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['Accuracy'] for m in results],\n",
        "    'Precision': [results[m]['Precision'] for m in results],\n",
        "    'Recall': [results[m]['Recall'] for m in results],\n",
        "    'F1': [results[m]['F1'] for m in results],\n",
        "    'AUC': [results[m]['AUC'] for m in results]\n",
        "})\n",
        "\n",
        "comp = comp.sort_values('F1', ascending=False).reset_index(drop=True)\n",
        "print(\"\\n\" + comp.to_string(index=False))\n",
        "\n",
        "best = comp.iloc[0]['Model']\n",
        "print(f\"\\nüèÜ BEST MODEL: {best}\")\n",
        "print(f\"   Accuracy:  {results[best]['Accuracy']*100:.2f}%\")\n",
        "print(f\"   Precision: {results[best]['Precision']*100:.2f}%\")\n",
        "print(f\"   Recall:    {results[best]['Recall']*100:.2f}%\")\n",
        "print(f\"   F1-Score:  {results[best]['F1']:.4f}\")\n",
        "print(f\"   ROC-AUC:   {results[best]['AUC']:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "if 'Random Forest' in trained:\n",
        "    feat_imp = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': trained['Random Forest'].feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(f\"\\nüìä Top 15 Features:\")\n",
        "    for i, r in feat_imp.head(15).iterrows():\n",
        "        print(f\"   {i+1:2d}. {r['Feature']:<40} {r['Importance']:.6f}\")\n",
        "\n",
        "    feat_imp.to_csv('feature_importance.csv', index=False)\n",
        "\n",
        "# Save models\n",
        "with open(f'best_model_{best.replace(\" \", \"_\").lower()}.pkl', 'wb') as f:\n",
        "    pickle.dump(trained[best], f)\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open('imputer.pkl', 'wb') as f:\n",
        "    pickle.dump(imputer, f)\n",
        "print(\"\\nüíæ Models saved\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 9: CREATING VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Model Comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Model Performance', fontsize=18, fontweight='bold')\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "for idx, m in enumerate(metrics):\n",
        "    ax = axes[idx//2, idx%2]\n",
        "    data = comp.sort_values(m, ascending=False)\n",
        "    ax.barh(data['Model'], data[m], color=['#e74c3c', '#3498db', '#2ecc71'])\n",
        "    ax.set_xlabel(m, fontsize=12, fontweight='bold')\n",
        "    ax.set_title(m, fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.grid(alpha=0.3, axis='x')\n",
        "    for i, v in enumerate(data[m]):\n",
        "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "print(\"   ‚úÖ model_comparison.png\")\n",
        "plt.close()\n",
        "\n",
        "# 2. Confusion Matrix\n",
        "cm = confusion_matrix(y_test, results[best]['Pred'])\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'],\n",
        "            cbar_kws={'label': 'Count'}, ax=ax, linewidths=2,\n",
        "            annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
        "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Confusion Matrix - {best}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "print(\"   ‚úÖ confusion_matrix.png\")\n",
        "plt.close()\n",
        "\n",
        "# 3. ROC Curves\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "for name in results:\n",
        "    if results[name]['Proba'] is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, results[name]['Proba'])\n",
        "        ax.plot(fpr, tpr, linewidth=3, label=f'{name} (AUC={results[name][\"AUC\"]:.3f})')\n",
        "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\n",
        "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "ax.set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='lower right', fontsize=11)\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "print(\"   ‚úÖ roc_curves.png\")\n",
        "plt.close()\n",
        "\n",
        "# 4. Feature Importance\n",
        "if 'Random Forest' in trained:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    top15 = feat_imp.head(15)\n",
        "    ax.barh(range(len(top15)), top15['Importance'],\n",
        "            color=plt.cm.viridis(np.linspace(0.3, 0.9, 15)))\n",
        "    ax.set_yticks(range(len(top15)))\n",
        "    ax.set_yticklabels(top15['Feature'], fontsize=10)\n",
        "    ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Top 15 Features', fontsize=14, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(alpha=0.3, axis='x')\n",
        "    for i, v in enumerate(top15['Importance']):\n",
        "        ax.text(v + 0.001, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"   ‚úÖ feature_importance.png\")\n",
        "    plt.close()\n",
        "\n",
        "# 5. Class Distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle('Class Distribution', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, (data, title) in enumerate([(y_train, 'Training'), (y_train_bal, 'After SMOTE'), (y_test, 'Validation')]):\n",
        "    axes[idx].bar(['Non-Fraud', 'Fraud'], [(data==0).sum(), (data==1).sum()],\n",
        "                  color=['#2ecc71', '#e74c3c'], edgecolor='black', linewidth=2)\n",
        "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Count', fontweight='bold')\n",
        "    axes[idx].grid(alpha=0.3, axis='y')\n",
        "    for i, v in enumerate([(data==0).sum(), (data==1).sum()]):\n",
        "        pct = v/len(data)*100\n",
        "        axes[idx].text(i, v+50, f'{v:,}\\n({pct:.1f}%)', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"   ‚úÖ class_distribution.png\")\n",
        "plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ PROJECT COMPLETED!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   Training: {len(train_final):,} providers, {X_train.shape[1]} features\")\n",
        "print(f\"   Validation: {len(test_final):,} providers\")\n",
        "\n",
        "print(f\"\\nüíæ Generated Files (10):\")\n",
        "print(f\"   üìä Analytics: 1 dashboard\")\n",
        "print(f\"   üìà ML Visualizations: 5 charts\")\n",
        "print(f\"   üìÅ Data & Models: 4 files\")\n",
        "\n",
        "print(f\"\\nüéì Mohammed Haqib (RA2512049015044)\")\n",
        "print(f\"üè´ SRM Institute of Science and Technology\")\n",
        "print(f\"üìÖ October 2025\")\n",
        "print(f\"\\n{'='*80}\\n\")\n"
      ]
    }
  ]
}